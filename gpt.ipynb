{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNa/4jvOwBUyKBQa2Vbiwfn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mervegb/nlp-sequence-prediction/blob/main/gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eCl9GpujadL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51bedd71-2666-4b19-ef7b-9f396f833599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-20 09:59:29--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-05-20 09:59:30 (18.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "BBRnqK0Geiib"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('length of dataset in characters', len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2LsaIzkeu-l",
        "outputId": "ea50eb36-73ed-44e5-8099-d3adee570c90"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_c4xrYDfIHR",
        "outputId": "11a60c04-edcc-410a-be9f-23dc90d23f92"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text))) #so like the possible characters model can see or emit\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF8chkwQfimC",
        "outputId": "1dab8d4d-ab9d-4dae-eb50-08c95f7f2573"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**\n",
        "\n",
        "breaking down larger body of text into smaller pieces called tokens\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G1vMSU3mgDua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create dictionaries for character-integer mapping\n",
        "#stoi => string to integer\n",
        "#itos => integer to string\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "print(itos)\n",
        "print(stoi)\n",
        "\n",
        "#these mappings are crucial for converting characters to integers and back, essentially\n",
        "#creating a lookup table for both"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukOBP5Bag5y9",
        "outputId": "875cf459-3544-4cab-f6f7-3cbddd057330"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
            "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lambda functions in python:\n",
        "#way to create small,anonymous functions, they don't need to be named\n",
        "add = lambda x,y: x+y\n",
        "print(add(5,3))\n",
        "\n",
        "numbers = [1,2,3,4,5]\n",
        "squared = list(map(lambda x: x**2, numbers))\n",
        "print(squared)\n",
        "\n",
        "even_numbers = list(filter(lambda x: x%2== 0, numbers))\n",
        "\n",
        "data = [('John', 45), ('Diane', 32), ('James', 28)]\n",
        "sorted_data = sorted(data, key=lambda x: x[1]) #sort by age\n",
        "print(sorted_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTrVEgGFjCDU",
        "outputId": "0023be3a-b1f9-4ccd-93ad-d16abb7f9500"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "[1, 4, 9, 16, 25]\n",
            "[('James', 28), ('Diane', 32), ('John', 45)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define the encoder function\n",
        "encode = lambda s: [stoi[c] for c in s] #encoder: takes a string, output list of integers\n",
        "\n",
        "#define the decoder function\n",
        "decode = lambda l: ''.join([itos[i] for i in l ]) #decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode('hii there')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbFz2HMliV02",
        "outputId": "511c41ec-7fbe-48dd-9d49-49b7c300b02e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tensor in Pytorch:\n",
        "#multi-dimensional array but with the added capability of being used on GPUs\n",
        "#and they integrate with gradient calculation, this makes them more suitable than NumPy arrays\n",
        "\n",
        "import torch\n",
        "\n",
        "#encode the entire text dataset and store it into torch.Tensor\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHmINCTylFvh",
        "outputId": "a58464fa-597f-4026-da57-d1bffe9f5a93"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split the data into train & validation sets\n",
        "n = int(0.9* len(data)) #first 90% will be the train and rest will be validation\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "#this will help us understand what extent our model is overfitting\n",
        "#we're going to hide & keep the validation data on side"
      ],
      "metadata": {
        "id": "4TROKEA8n5Dy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8 #amount of data processed at one time\n",
        "#it defines how many elements of training data to look at one time\n",
        "\n",
        "#too-small block size might not provide enough context for accurate predictions\n",
        "#too-large block size might make model less generalizable\n",
        "\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot16qRIup7F0",
        "outputId": "67f35895-7dd0-4399-cded-f1e42d2e21e8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size] # train_data[:8] - tensor([18, 47, 56, 57, 58, 1, 15, 47])\n",
        "y= train_data[1: block_size + 1] # train_data[1:9] tensor([47, 56, 57, 58, 1, 15, 47, 58])\n",
        "\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target: {target}\")\n",
        "\n",
        "\n",
        "#y is the next element following the sequence in x\n",
        "#this setup is for predicting the next character in a sentence\n",
        "#by iterating over block_size, each context becomes a training example where the model learns that given context it should predict the target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "solMgJHfq0wO",
        "outputId": "fc19d90e-ac8a-4a0c-f868-1b514fc3ba81"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch Size**\n",
        "\n",
        "number of training examples used to train single iteration of a model\n",
        "When you train a model you typically don't pass the entire dataset through the network at once (computational efficiency & memory limitations)\n",
        "Instead you divide the dataset into smaller sets of data called batches\n",
        "\n",
        "\n",
        "**Example**:\n",
        "Batch Size: Suppose you set it to 32. This means 32 sequences are processed in parallel during each training step.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Block Size**\n",
        "\n",
        "sequence length\n",
        "determines how many previous words or tokens the model considers before making prediction\n",
        "\n",
        "how many past data points are used to predict future values\n",
        "\n",
        "maximum context length for predictions, this is relevant in models that generate text or make predictions based on past information\n",
        "\n"
      ],
      "metadata": {
        "id": "JhPgqcjma2OD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337) #to get same sequence of random numbers\n",
        "batch_size = 4 #the number of sequences in each batch\n",
        "block_size = 8 #the number of items in each sequence\n",
        "\n",
        "#if batch_size is 4 it means every batch generated will contain 4 separate sequences\n",
        "#each of the ix will be starting point for a sequence in your data\n",
        "\n",
        "#if block_size is 8 each sequence extracted will have 8 data points\n",
        "\n",
        "#len(data) -> 1.115.394\n",
        "#len(data) - block_size => upper boundary of range non inclusive\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,)) #batch_size -> will generate 4 random integers\n",
        "\n",
        "  #[data[i: i + block_size] will take 8 consecutive elements from data, starting from index i\n",
        "  x = torch.stack([data[i: i + block_size] for i in ix]) #x[76049] = 24)\n",
        "  y = torch.stack([data[i+1: i + block_size+1] for i in ix])\n",
        "  return x,y\n",
        "\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "\n",
        "print('targets')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): #iterates over each sequence in the batch\n",
        "  for t in range(block_size): #iterates over each element within sequence\n",
        "    context = xb[b,:t+1]\n",
        "    target = yb[b,t]\n",
        "    print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "id": "j7frtrLgu5Pe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e0ca63-3806-4931-bcc9-f531096c2c5a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bigram Language Model**\n",
        "\n",
        "predicts the next word in a sequence based on the previous word\n",
        "\n",
        "- Bigram models are limited by lack of context; they can only look one word back, this can lead to less accurate predictions in complex texts where more context is necessary\n",
        "\n",
        "- Zero probability problem => if word has never occured in the training data the model won't be able to predict"
      ],
      "metadata": {
        "id": "iYcg8LbMoxB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embedding Lookup**\n",
        "\n",
        "each word in a sequence is mapped to its corresponding vector representation from an embedding matrix, this vector representation captures semantic mearning too"
      ],
      "metadata": {
        "id": "_ehyGztxLvMC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reshaping is done to prepare the tensors for the loss computation\n",
        "Cross entropy loss expects the input tensor to have shape (N,C) and target to have shape (N) where N is the total number of predictions (B*T)\n",
        "\n",
        "B: batch size\n",
        "\n",
        "T: sequence length\n",
        "\n",
        "C: number of classes (vocabulary size)"
      ],
      "metadata": {
        "id": "88nLF0l7b04a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    #initialize embedding layer\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    logits = self.token_embedding_table(idx) #embedding lookup\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T,C) #reshapes logits from (B,T,C) to (B*T,C) to flatten the batch and sequence dimensions into one\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  #this method generates new tokens\n",
        "  #idx tensor of shape (B,T) representing the current context, B batch size, T sequence length\n",
        "  #max_new_tokens number of new tokens to generate\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx) #calls the forward pass to get predictions and loss\n",
        "      logits = logits[:,-1,:] #becomes (B,C)\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) #(B,1) this contains the indices of the newly generated tokens\n",
        "      idx = torch.cat((idx, idx_next), dim=1) #extends the current sequences by adding the newly generated tokens (B,T+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size) #creates instance of the model\n",
        "logits, loss = m(xb, yb) #performs forward pass, where xb is input tensor & yb is target tensor\n",
        "\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "idx = torch.zeros((1,1), dtype=torch.long)\n",
        "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "id": "1k_onb6ApuJO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45513374-0117-4df6-9e2a-9485dbcf488b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "1fPaG-GmzFGZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32 #each batch will contain 32 samples\n",
        "\n",
        "#Training loop\n",
        "for steps in range(10000):\n",
        "  #sample batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  #evaluate the loss\n",
        "  logits, loss = m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True) #clears old gradients from previous steps\n",
        "  loss.backward() #performs backpropagation\n",
        "  optimizer.step() #updates model's parameters using the computed gradients\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4GoieZLzNIZ",
        "outputId": "7256c2d3-a887-4e1d-bc79-adfb5a3a1afd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5727508068084717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx, max_new_tokens=300)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOzO54ug2HhA",
        "outputId": "0fdd294b-cd04-4414-f32b-fe710ead3836"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iyoteng h hasbe pave pirance\n",
            "Rie hicomyonthar's\n",
            "Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n",
            "KIN d pe wither vouprrouthercc.\n",
            "hathe; d!\n",
            "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n",
            "h hay.JUCle n prids, r loncave w hollular s O:\n",
            "HIs; ht \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mathematical Trick in Self-Attention"
      ],
      "metadata": {
        "id": "gZ1d8zjndVIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In self-attention, we often need to compute attention weights that decide how much focus each token should give to every other token in sequence. These attention weights are used to compute a weighted sum of values.\n",
        "\n",
        "<br />\n",
        "Lower triangular matrix is often used to implement autoregressive attention. This ensures that each token can only attend to previous tokens and itself not to future tokens. This is crucial in tasks like language modeling where prediction of a token should not depend on future tokens\n",
        "\n",
        "<br />\n",
        "\n",
        "Normalizing the rows so that they sum to 1 is to apply softmax function to the attention scores."
      ],
      "metadata": {
        "id": "FvvoauDVjc4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "id": "jngvkA0UE23s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "404cc12c-9f3d-4c8b-ff14-e95302462c19"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time steps, features/channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLREqrfCsMIS",
        "outputId": "e18dbcfb-4725-4164-bfc6-99cc75bdfe8a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#version 1: nested loop with mean\n",
        "#directly computes the mean of all previous time steps for each time step using nested loops\n",
        "#simple but less efficient for large tensors\n",
        "\n",
        "xbow = torch.zeros((B,T,C))\n",
        "\n",
        "#compute xbow using nested looğs\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # select all time steps up to and including t\n",
        "        xbow[b,t] = torch.mean(xprev, 0) #compute the mean along the time dimension\n",
        "print(xbow[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2uztkEssNvK",
        "outputId": "f945a6b7-5f13-4a52-a66d-cf88a268a792"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.0894, -0.4926],\n",
            "        [ 0.1490, -0.3199],\n",
            "        [ 0.3504, -0.2238],\n",
            "        [ 0.3525,  0.0545],\n",
            "        [ 0.0688, -0.0396],\n",
            "        [ 0.0927, -0.0682],\n",
            "        [-0.0341,  0.1332]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "\n",
        "#uses lower triangular to achieve same result as version 1\n",
        "#efficient due to matrix multiplication but requires setup of the weight matrix\n",
        "\n",
        "#create and normalize lower triangular matrix\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "\n",
        "#compute xbow2 using matrix multiplication\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "\n",
        "#check if xbow and xbow2 are approximately equal\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPL3EzAZsPZI",
        "outputId": "8aa050a3-4fab-4b04-fe81-87ceba56d589"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "#similar to version 2 but uses the softmax function to normalize weights\n",
        "#more generalized and handles the weighted aggregation through learned weights\n",
        "\n",
        "#create lower triangular mask\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "\n",
        "#initialize weight matrix with -inf where mask is 0\n",
        "wei = torch.zeros((T,T))\n",
        "\n",
        "#masked_fill => fills elements of self tensor with value where the mask is True\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "print(wei)\n",
        "\n",
        "#apply softmax to get weights\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "#compute xbow3 using matrix multiplication\n",
        "xbow3 = wei @ x\n",
        "\n",
        "torch.allclose(xbow, xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10viiWHesX6L",
        "outputId": "7b75b0ca-b5ad-415f-e5b6-289768b26908"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ]
}