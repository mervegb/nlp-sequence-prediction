{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtI16E2eMfIfLtKWGosvof",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mervegb/nlp-sequence-prediction/blob/main/gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eCl9GpujadL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c57af4a-bd7a-4570-bf45-d40857435879"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-05 09:48:00--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-05-05 09:48:01 (18.6 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "BBRnqK0Geiib"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('length of dataset in characters', len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2LsaIzkeu-l",
        "outputId": "b5b28b64-40d7-4635-a95e-67f630afdd17"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_c4xrYDfIHR",
        "outputId": "8ecfd1a9-a85f-4b5e-b570-027157a3dc40"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text))) #so like the possible characters model can see or emit\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF8chkwQfimC",
        "outputId": "048e3a4b-d2f7-4d93-e05f-1a5e1a9c55f3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**\n",
        "\n",
        "breaking down larger body of text into smaller pieces called tokens\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G1vMSU3mgDua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create dictionaries for character-integer mapping\n",
        "#stoi => string to integer\n",
        "#itos => integer to string\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "print(itos)\n",
        "print(stoi)\n",
        "\n",
        "#these mappings are crucial for converting characters to integers and back, essentially\n",
        "#creating a lookup table for both"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukOBP5Bag5y9",
        "outputId": "fae3dd52-9dbb-431a-c0d0-0bb582a9e407"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
            "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lambda functions in python:\n",
        "#way to create small,anonymous functions, they don't need to be named\n",
        "add = lambda x,y: x+y\n",
        "print(add(5,3))\n",
        "\n",
        "numbers = [1,2,3,4,5]\n",
        "squared = list(map(lambda x: x**2, numbers))\n",
        "print(squared)\n",
        "\n",
        "even_numbers = list(filter(lambda x: x%2== 0, numbers))\n",
        "\n",
        "data = [('John', 45), ('Diane', 32), ('James', 28)]\n",
        "sorted_data = sorted(data, key=lambda x: x[1]) #sort by age\n",
        "print(sorted_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTrVEgGFjCDU",
        "outputId": "2d78283e-ddea-48e2-ad82-5defd48f47cd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "[1, 4, 9, 16, 25]\n",
            "[('James', 28), ('Diane', 32), ('John', 45)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define the encoder function\n",
        "encode = lambda s: [stoi[c] for c in s] #encoder: takes a string, output list of integers\n",
        "\n",
        "#define the decoder function\n",
        "decode = lambda l: ''.join([itos[i] for i in l ]) #decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode('hii there')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbFz2HMliV02",
        "outputId": "42b1554f-7b3c-4a14-e29a-0c99e804e05a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tensor in Pytorch:\n",
        "#multi-dimensional array but with the added capability of being used on GPUs\n",
        "#and they integrate with gradient calculation, this makes them more suitable than NumPy arrays\n",
        "\n",
        "import torch\n",
        "\n",
        "#encode the entire text dataset and store it into torch.Tensor\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHmINCTylFvh",
        "outputId": "ecccbe8b-7c84-40d7-a502-22171e14f2b0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split the data into train & validation sets\n",
        "n = int(0.9* len(data)) #first 90% will be the train and rest will be validation\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "#this will help us understand what extent our model is overfitting\n",
        "#we're going to hide & keep the validation data on side"
      ],
      "metadata": {
        "id": "4TROKEA8n5Dy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8 #amount of data processed at one time\n",
        "#it defines how many elements of training data to look at one time\n",
        "\n",
        "#too-small block size might not provide enough context for accurate predictions\n",
        "#too-large block size might make model less generalizable\n",
        "\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot16qRIup7F0",
        "outputId": "af6ea57f-d434-4f32-8a4a-e55d9f8d331e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size] # train_data[:8] - tensor([18, 47, 56, 57, 58, 1, 15, 47])\n",
        "y= train_data[1: block_size + 1] # train_data[1:9] tensor([47, 56, 57, 58, 1, 15, 47, 58])\n",
        "\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target: {target}\")\n",
        "\n",
        "\n",
        "#y is the next element following the sequence in x\n",
        "#this setup is for predicting the next character in a sentence\n",
        "#by iterating over block_size, each context becomes a training example where the model learns that given context it should predict the target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "solMgJHfq0wO",
        "outputId": "bef0078b-df33-4110-d102-583c096b1172"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch Size**\n",
        "\n",
        "number of training examples used to train single iteration of a model\n",
        "When you train a model you typically don't pass the entire dataset through the network at once (computational efficiency & memory limitations)\n",
        "Instead you divide the dataset into smaller sets of data called batches\n",
        "\n",
        "\n",
        "**Example**:\n",
        "Batch Size: Suppose you set it to 32. This means 32 sequences are processed in parallel during each training step.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Block Size**\n",
        "\n",
        "sequence length\n",
        "determines how many previous words or tokens the model considers before making prediction\n",
        "\n",
        "how many past data points are used to predict future values\n",
        "\n",
        "maximum context length for predictions, this is relevant in models that generate text or make predictions based on past information\n",
        "\n"
      ],
      "metadata": {
        "id": "JhPgqcjma2OD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337) #to get same sequence of random numbers\n",
        "batch_size = 4 #the number of sequences in each batch\n",
        "block_size = 8 #the number of items in each sequence\n",
        "\n",
        "#if batch_size is 4 it means every batch generated will contain 4 separate sequences\n",
        "#each of the ix will be starting point for a sequence in your data\n",
        "\n",
        "#if block_size is 8 each sequence extracted will have 8 data points\n",
        "\n",
        "#len(data) -> 1.115.394\n",
        "#len(data) - block_size => upper boundary of range non inclusive\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,)) #batch_size -> will generate 4 random integers\n",
        "\n",
        "  #[data[i: i + block_size] will take 8 consecutive elements from data, starting from index i\n",
        "  x = torch.stack([data[i: i + block_size] for i in ix]) #x[76049] = 24)\n",
        "  y = torch.stack([data[i+1: i + block_size+1] for i in ix])\n",
        "  return x,y\n",
        "\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "\n",
        "print('targets')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): #iterates over each sequence in the batch\n",
        "  for t in range(block_size): #iterates over each element within sequence\n",
        "    context = xb[b,:t+1]\n",
        "    target = yb[b,t]\n",
        "    print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "id": "j7frtrLgu5Pe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c684ac18-de41-4beb-9eac-91c464648e9c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bigram Language Model**\n",
        "\n",
        "predicts the next word in a sequence based on the previous word\n",
        "\n",
        "- Bigram models are limited by lack of context; they can only look one word back, this can lead to less accurate predictions in complex texts where more context is necessary\n",
        "\n",
        "- Zero probability problem => if word has never occured in the training data the model won't be able to predict"
      ],
      "metadata": {
        "id": "iYcg8LbMoxB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward Pass**\n",
        "\n",
        "compute outputs that can be used for prediction and calculate the loss\n",
        "\n",
        "<br/>\n",
        "\n",
        "**Embedding Lookup**\n",
        "\n",
        "retrieving an embedding vector from an embedding matrix based on an index or set of indices"
      ],
      "metadata": {
        "id": "_ehyGztxLvMC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding B, T, C**\n",
        "\n",
        "B: Batch Size\n",
        "\n",
        "T: Sequence Length\n",
        "\n",
        "C: Features\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lOEd1DToOvuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    #each token directly reads logits for the next token from lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  #idx -> tensor of shape (B,T)\n",
        "  #B = batch size, T = sequence length\n",
        "  #targets = target indices, same shape as idx\n",
        "  def forward(self, idx, targets=None):\n",
        "    logits = self.token_embedding_table(idx) #embedding lookup\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1k_onb6ApuJO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "f97027a4-d5c4-4874-efe1-f4014e13b1a1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-14-a2fe5df3e711>, line 15)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-a2fe5df3e711>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**n.Module** is the base class for all neural networks modules\n",
        "\n",
        "<br/>\n",
        "\n",
        "**Input Layer**\n",
        "\n",
        "responsible for receiving input data and passing it on to the next layer\n",
        "\n",
        "<br/>\n",
        "\n",
        "**Hidden Layer**\n",
        "\n",
        "can be found in almost every type of neural network except layers like perceptron.\n",
        "\n",
        "<br/>\n",
        "\n",
        "**Output Layer**\n",
        "\n",
        "last layer which produces the final output or prediction\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "**Embedding Layer**\n",
        "\n",
        "type of hidden layer in neural network\n",
        "\n",
        "this layer maps input information from high dimensional to lower dimensional space, allowing network to learn more about the relationship between inputs\n",
        "\n",
        "Embeddings reduce the dimensionality of categorical variables compared to one-hot encoding which can be extremely sparse and inefficient\n",
        "\n"
      ],
      "metadata": {
        "id": "1yleS8SWq34U"
      }
    }
  ]
}